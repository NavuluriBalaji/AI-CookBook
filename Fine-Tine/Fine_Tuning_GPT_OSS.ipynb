{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e61195f-1dea-4841-8244-9c3da7c74710",
      "metadata": {
        "id": "8e61195f-1dea-4841-8244-9c3da7c74710",
        "outputId": "30a1b9b4-1e68-4f35-9173-ae9ea5907d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \"trl>=0.20.0\" \"peft>=0.17.0\" \"transformers>=4.55.0\" trackio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf58cc4-b4a4-461b-b1c2-46c0ec5faa8f",
      "metadata": {
        "id": "9bf58cc4-b4a4-461b-b1c2-46c0ec5faa8f",
        "outputId": "eb576ebb-7c34-4ac5-e369-adb7a6e46fd9",
        "colab": {
          "referenced_widgets": [
            "319cd74947e8451bb64a0e32db273827"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "319cd74947e8451bb64a0e32db273827",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f42f4826-2ace-4e6e-828b-671f2711c85c",
      "metadata": {
        "id": "f42f4826-2ace-4e6e-828b-671f2711c85c",
        "outputId": "6f91f609-2c97-4392-acb9-41a149669367"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['reasoning_language', 'developer', 'user', 'analysis', 'final', 'messages'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee54ca1d-f3ad-403a-b7d5-33c34b248e95",
      "metadata": {
        "id": "ee54ca1d-f3ad-403a-b7d5-33c34b248e95"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28209f85-72d4-497a-a576-8a67de69d5df",
      "metadata": {
        "id": "28209f85-72d4-497a-a576-8a67de69d5df",
        "outputId": "83410cfa-a869-4854-b392-70806ea6c195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
            "Knowledge cutoff: 2024-06\n",
            "Current date: 2025-08-06\n",
            "\n",
            "Reasoning: medium\n",
            "\n",
            "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
            "\n",
            "reasoning language: French\n",
            "\n",
            "You are an AI chatbot with a lively and energetic personality.<|end|><|start|>user<|message|>Can you show me the latest trends on Twitter right now?<|end|><|start|>assistant<|channel|>analysis<|message|>D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\n",
            "\n",
            "Je devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section « En vogue » sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.<|end|><|start|>assistant<|channel|>final<|message|>Hey there!  While I can't check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\n",
            "\n",
            "1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \n",
            "2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \n",
            "3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \n",
            "4. **Check regional trends** – they often differ by location!  \n",
            "\n",
            "Remember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \n",
            "\n",
            "Want me to brainstorm *what* might trend next? I’ve got ideas!<|return|>\n"
          ]
        }
      ],
      "source": [
        "messages = dataset[0][\"messages\"]\n",
        "conversation = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(conversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "999fd36d-0e5a-430d-ac09-840bac79f8d2",
      "metadata": {
        "id": "999fd36d-0e5a-430d-ac09-840bac79f8d2",
        "outputId": "3ad12568-0ff3-4887-eec2-2f43a0513f63",
        "colab": {
          "referenced_widgets": [
            "0a7f40c9ceaf43758de1eb554ada16c3"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a7f40c9ceaf43758de1eb554ada16c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, Mxfp4Config\n",
        "\n",
        "quantization_config = Mxfp4Config(dequantize=True)\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=quantization_config,\n",
        "    use_cache=False,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai/gpt-oss-20b\", **model_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f2c00d6-f5fd-4bd9-b8c4-6acfa388b59b",
      "metadata": {
        "id": "4f2c00d6-f5fd-4bd9-b8c4-6acfa388b59b",
        "outputId": "3a016885-2649-4307-c766-47d4bcc2d89e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
            "Knowledge cutoff: 2024-06\n",
            "Current date: 2025-08-06\n",
            "\n",
            "Reasoning: medium\n",
            "\n",
            "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>¿Cuál es el capital de Australia?<|end|><|start|>assistant<|channel|>analysis<|message|>User asks in Spanish: \"¿Cuál es el capital de Australia?\" They want Canberra. Response: simply answer.<|end|><|start|>assistant<|channel|>final<|message|>La capital de Australia es Canberra.<|return|>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"¿Cuál es el capital de Australia?\"},\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "output_ids = model.generate(input_ids, max_new_tokens=512)\n",
        "response = tokenizer.batch_decode(output_ids)[0]\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16ee4c92-ac3e-4015-92ba-85e5b9862215",
      "metadata": {
        "id": "16ee4c92-ac3e-4015-92ba-85e5b9862215",
        "outputId": "02226dbf-537b-4b15-e962-9266616a6b5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:159: UserWarning: Unsupported layer type '<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssExperts'>' encountered, proceed at your own risk.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 15,040,512 || all params: 20,929,797,696 || trainable%: 0.0719\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=\"all-linear\",\n",
        "    target_parameters=[\n",
        "        \"7.mlp.experts.gate_up_proj\",\n",
        "        \"7.mlp.experts.down_proj\",\n",
        "        \"15.mlp.experts.gate_up_proj\",\n",
        "        \"15.mlp.experts.down_proj\",\n",
        "        \"23.mlp.experts.gate_up_proj\",\n",
        "        \"23.mlp.experts.down_proj\",\n",
        "    ],\n",
        ")\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e063c71-2f25-41a3-91ae-9592acd2d387",
      "metadata": {
        "id": "1e063c71-2f25-41a3-91ae-9592acd2d387"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    learning_rate=2e-4,\n",
        "    gradient_checkpointing=True,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_length=2048,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
        "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
        "    output_dir=\"gpt-oss-20b-multilingual-reasoner\",\n",
        "    report_to=\"trackio\",\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfdb72d8-8d2e-4a04-877a-dcc8bcd89638",
      "metadata": {
        "id": "bfdb72d8-8d2e-4a04-877a-dcc8bcd89638",
        "outputId": "54056e06-41cf-4a0e-e067-8f7182a6ab27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Trackio project initialized: huggingface\n",
            "* Trackio metrics logged to: /root/.cache/huggingface/trackio\n",
            "* View dashboard by running in your terminal:\n",
            "\u001b[1m\u001b[93mtrackio show --project \"huggingface\"\u001b[0m\n",
            "* or by running in Python: trackio.show(project=\"huggingface\")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 19:43, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.963300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.037800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.806800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.826500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.605000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.566600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.411000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.412500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.214000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.320700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.311800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.237500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.164600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.144400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.231500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.157100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.236800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.134600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.095600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.119400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.051200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.056000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.044700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.031900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.097600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.093100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.892900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.034700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.035800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.030900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.042200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.061800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.010400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.114700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.086300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.943400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.043700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.960800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.015100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.996800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.979700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.878700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.920700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.091800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.047200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.113000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.988900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.870400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.063500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.936300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.005800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.069200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.993700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.030200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.089400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.119300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.023500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.018300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.991800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.937600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.995800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.095100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=63, training_loss=1.1413098110093012, metrics={'train_runtime': 1204.2656, 'train_samples_per_second': 0.83, 'train_steps_per_second': 0.052, 'total_flos': 1.9951547028611328e+17, 'train_loss': 1.1413098110093012})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c5fb4c5-be17-4d4a-a3b8-d17fceabce7d",
      "metadata": {
        "id": "1c5fb4c5-be17-4d4a-a3b8-d17fceabce7d",
        "outputId": "bbdb6b5a-fb66-41ca-c8e9-0ce1f7f56978"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/VizuaraAI/gpt-oss-20b-multilingual-reasoner/commit/10037253f3336e8d06dd55696386d8e3e804c03c', commit_message='End of training', commit_description='', oid='10037253f3336e8d06dd55696386d8e3e804c03c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/VizuaraAI/gpt-oss-20b-multilingual-reasoner', endpoint='https://huggingface.co', repo_type='model', repo_id='VizuaraAI/gpt-oss-20b-multilingual-reasoner'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.save_model(training_args.output_dir)\n",
        "trainer.push_to_hub(dataset_name=\"VizuaraAI/Multilingual-Thinking\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb28ed4e-513a-40f3-97ac-de92f54fdcaf",
      "metadata": {
        "id": "bb28ed4e-513a-40f3-97ac-de92f54fdcaf",
        "outputId": "f0124378-196e-4aa9-9f52-95e52383b61a",
        "colab": {
          "referenced_widgets": [
            "87d7ccf6b1dd4612aea6df632d315782"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MXFP4 quantization requires triton >= 3.4.0 and triton_kernels installed, we will default to dequantizing the model to bf16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87d7ccf6b1dd4612aea6df632d315782",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:159: UserWarning: Unsupported layer type '<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssExperts'>' encountered, proceed at your own risk.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
        "\n",
        "# Load the original model first\n",
        "model_kwargs = dict(attn_implementation=\"eager\", torch_dtype=\"auto\", use_cache=True, device_map=\"auto\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"openai/gpt-oss-20b\", **model_kwargs).cuda()\n",
        "\n",
        "# Merge fine-tuned weights with the base model\n",
        "peft_model_id = \"gpt-oss-20b-multilingual-reasoner\"\n",
        "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
        "model = model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04f2517d-fb7d-4e58-b613-a17a2a588f09",
      "metadata": {
        "id": "04f2517d-fb7d-4e58-b613-a17a2a588f09",
        "outputId": "3a771793-daaa-4108-f423-42dcdf92852b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
            "Knowledge cutoff: 2024-06\n",
            "Current date: 2025-08-06\n",
            "\n",
            "Reasoning: medium\n",
            "\n",
            "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
            "\n",
            "reasoning language: German<|end|><|start|>user<|message|>¿Cuál es el capital de Australia?<|end|><|start|>assistant<|channel|>analysis<|message|>\n",
            "In Ordnung, der Benutzer fragt nach der Hauptstadt Australiens. Ich sollte zuerst sicherstellen, dass ich die richtige Hauptstadt kenne. Ich glaube, es ist Canberra, aber ich sollte das überprüfen. Ich erinnere mich, dass Sydney und Melbourne die größten Städte sind, aber die Hauptstadt ist anders. Canberra wurde in den 1920er Jahren ausgewählt, um die beiden Städte zu trennen. Es ist ein geplantes Stadtgebiet, nicht die größte Stadt. Der Benutzer könnte aus einer Region stammen, in der Canberra weniger bekannt ist, daher ist es wichtig, es klar zu benennen. Vielleicht erwähne ich auch, dass es nicht die größte Stadt ist, um Missverständnisse zu vermeiden. Ich sollte die Antwort kurz halten, da die Frage direkt ist. Ich sollte keine unnötigen Details hinzufügen und einfach die Hauptstadt nennen. Der Benutzer wollte vermutlich nur die Hauptstadt wissen. Also: Canberra.<|end|><|start|>assistant<|channel|>final<|message|>The capital of Australia is Canberra.<|return|>\n"
          ]
        }
      ],
      "source": [
        "REASONING_LANGUAGE = \"German\"\n",
        "SYSTEM_PROMPT = f\"reasoning language: {REASONING_LANGUAGE}\"\n",
        "USER_PROMPT = \"¿Cuál es el capital de Australia?\"  # Spanish for \"What is the capital of Australia?\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": USER_PROMPT},\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "gen_kwargs = {\"max_new_tokens\": 512, \"do_sample\": True, \"temperature\": 0.6, \"top_p\": None, \"top_k\": None}\n",
        "\n",
        "output_ids = model.generate(input_ids, **gen_kwargs)\n",
        "response = tokenizer.batch_decode(output_ids)[0]\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd8b0c25-7f64-4303-a48c-173843aaace1",
      "metadata": {
        "id": "cd8b0c25-7f64-4303-a48c-173843aaace1",
        "outputId": "32deacf5-30e9-44f7-9480-5dfbbd1ecf7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
            "Knowledge cutoff: 2024-06\n",
            "Current date: 2025-08-06\n",
            "\n",
            "Reasoning: medium\n",
            "\n",
            "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
            "\n",
            "reasoning language: Chinese<|end|><|start|>user<|message|>What is the national symbol of Canada?<|end|><|start|>assistant<|channel|>analysis<|message|>首先，我得考虑加拿大的国旗。它以红色和白色为主，中央有一片红色的枫叶。枫叶在加拿大文化中非常重要，象征着国家的自然环境和历史。枫树在加拿大的北部和中部地区非常常见，枫叶的形状也很独特，容易识别。\n",
            "\n",
            "此外，加拿大的国旗在设计上也很有特色。红色的条纹代表加拿大的历史和文化，白色的背景象征着和平与和谐。枫叶的形状和颜色也很突出，给人一种强烈的视觉冲击。\n",
            "\n",
            "我还记得，枫叶在加拿大的许多官方文件和象征中都有出现，例如加拿大的国徽、国旗、国旗、以及许多公共场所的标识。枫叶也经常出现在加拿大的文化和艺术中，成为加拿大的象征之一。\n",
            "\n",
            "所以，我可以确定加拿大的国旗上的枫叶是国家的象征。它代表了加拿大的自然环境、文化、历史和精神。枫叶的形状和颜色具有独特性和辨识度，成为加拿大的独特标志。\n",
            "\n",
            "我还要确保我没有遗漏其他可能的国家象征，例如加拿大的国徽、国旗、国旗、国旗等。但我认为枫叶是最主要和最具代表性的象征。它在加拿大的文化和历史中占有重要地位，受到广泛认可和尊重。\n",
            "\n",
            "我还要检查一下枫叶是否在加拿大的官方文件和象征中出现过，例如加拿大的国旗、国旗、国旗、国旗、国旗、国旗、国旗、国旗、国旗、国旗、国旗、国旗、国旗、国旗等。枫叶的形状和颜色也很突出，给人一种强烈的视觉冲击。\n",
            "\n",
            "所以，我的答案是加拿大的国旗上的枫叶。它是加拿大的象征，代表了国家的自然环境、文化和历史。枫叶的形状和颜色具有独特性和辨识度，成为加拿大的独特标志。<|end|><|start|>assistant<|channel|>final<|message|>The national symbol of Canada is the **red maple leaf**. It is featured prominently on the Canadian flag, which has a red maple\n"
          ]
        }
      ],
      "source": [
        "\n",
        "REASONING_LANGUAGE = \"Chinese\"  # or Hindi, or any other language...\n",
        "SYSTEM_PROMPT = f\"reasoning language: {REASONING_LANGUAGE}\"\n",
        "USER_PROMPT = \"What is the national symbol of Canada?\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": USER_PROMPT},\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "output_ids = model.generate(input_ids, **gen_kwargs)\n",
        "response = tokenizer.batch_decode(output_ids)[0]\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a31f88f-52ab-4270-b099-5919784f2c3e",
      "metadata": {
        "id": "4a31f88f-52ab-4270-b099-5919784f2c3e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}